{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import math\n",
    "import pathlib\n",
    "import re\n",
    "\n",
    "import tqdm\n",
    "\n",
    "import rosbag\n",
    "import geometry_msgs.msg as geomsg\n",
    "import message_filters\n",
    "\n",
    "import cameratransform as ct\n",
    "from tf.transformations import quaternion_from_euler, euler_from_quaternion\n",
    "from scipy.spatial.transform import Rotation\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "from cv_bridge import CvBridge\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import av\n",
    "import postprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GazeboRosPkgs\t\t     Scripts_deprecated\t\t       nrp_installer.sh\r\n",
      "Models\t\t\t     TODO.md\t\t\t       pkgs.txt\r\n",
      "NRPExp_DVSDatabaseGenerator  data\t\t\t       record.bag\r\n",
      "README.md\t\t     gazebo_actor-updaterate_patch.sh  src\r\n"
     ]
    }
   ],
   "source": [
    "!ls .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "bag = rosbag.Bag(\"../record.bag\")\n",
    "model_topic = \"/gazebo_modelstates_with_timestamp\"\n",
    "camera_topic = \"/robot/camera_rgb_00\"\n",
    "event_topic = \"/robot/camera_dvs_00/events\"\n",
    "bridge = CvBridge()\n",
    "g = postprocess.process_bag(bag, bridge, model_topic, camera_topic, event_topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "message_filters.ApproximateTimeSynchronizer([bag.read_messages(topics=[model_topic])], 10, 0.1, allow_headerless=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hasattr(msg, 'header')\n",
    "datetime.time(second=msg.header.stamp.secs, microsecond=msg.header.stamp.nsecs // 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datetime.time(second=t.secs, microsecond=t.nsecs // 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for topic, msg, t in bag.read_messages(topics=[model_topic]):\n",
    "    #msg.data = None\n",
    "    print(t, msg.gazebo_model_states)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for topic, msg, t in bag.read_messages(topics=[\"/gazebo/link_states\"]):\n",
    "#     d = {n: i for i, n in enumerate(msg.name)}\n",
    "#     i = d[\"animated camera::eye_vision_camera\"]\n",
    "#     print(msg.pose[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_labels(camera, tool_poses, tool_meshes):\n",
    "    images = []\n",
    "    for tool_class, (tool, pose) in enumerate(tool_poses.items()):\n",
    "        raw_vertices, raw_triangles = tool_meshes[tool]\n",
    "        transformed = transform_tool(raw_vertices, pose)\n",
    "        projection = camera.imageFromSpace(transformed)\n",
    "        triangles = np.take(projection, raw_triangles, axis=0).astype(int)\n",
    "        image_class = np.zeros((512, 512))\n",
    "        for mesh in triangles:\n",
    "            cv2.fillConvexPoly(image_class, mesh, tool_class + 1)\n",
    "        images.append(image_class)\n",
    "    return np.stack(images, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_tool(mesh, pose):\n",
    "    orientation = pose.orientation\n",
    "    orientation_quat = np.array(\n",
    "        [orientation.x, orientation.y, orientation.z, orientation.w]\n",
    "    )\n",
    "    rotation = Rotation.from_quat(orientation_quat)\n",
    "    rotation_corrected = rotation.as_euler('xyz')\n",
    "    rotation = Rotation.from_euler('xyz', rotation_corrected)\n",
    "    position = np.array([pose.position.x, pose.position.y, pose.position.z])\n",
    "    return position + rotation.apply(mesh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def camera_transform_generator(pose, focal_length=0.5003983220157445 * 512):\n",
    "    for t, (x, y, z, roll, pitch, yaw) in camera_pose_generator:\n",
    "        (roll, pitch, yaw) = (radians / 180 * np.pi for radians in (roll, pitch, yaw))\n",
    "        projection = ct.RectilinearProjection(\n",
    "            focallength_px=focal_length, image=(512, 512)\n",
    "        )\n",
    "        orientation = ct.SpatialOrientation(\n",
    "            pos_x_m=x,\n",
    "            pos_y_m=y,\n",
    "            elevation_m=z,\n",
    "            roll_deg=roll, # TODO: Specify\n",
    "            tilt_deg=90 - pitch,\n",
    "            heading_deg=90 - yaw,\n",
    "        )\n",
    "        yield t, ct.Camera(projection, orientation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = next(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(image, camera, camera_pose, tool_poses, tool_meshes, labels, event, times) = out\n",
    "(x, y, z, roll, pitch, yaw) = camera_pose\n",
    "(roll, pitch, yaw) = (radians * 180 / np.pi for radians in (roll, pitch, yaw))\n",
    "projection = ct.RectilinearProjection(\n",
    "   focallength_px=0.5003983220157445 * 512, image=(512, 512)\n",
    ")\n",
    "orientation = ct.SpatialOrientation(\n",
    "   pos_x_m=x, pos_y_m=y, elevation_m=z,\n",
    "   roll_deg=roll, tilt_deg=90-pitch, heading_deg=90-yaw,\n",
    ")\n",
    "camera2 = ct.Camera(projection, orientation)\n",
    "label = image_labels(camera2, tool_poses, tool_meshes)\n",
    "print(times)\n",
    "#plt.imshow(label / 3)\n",
    "plt.imshow(image / 400 + label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 8))\n",
    "(image, events, labels) = out\n",
    "#images = image_labels(camera, tool_poses, tool_meshes)\n",
    "#cl = images[0] + images[1] + images[2]\n",
    "#cl = np.broadcast_to(np.expand_dims(cl, 2), (512, 512, 3))\n",
    "plt.imshow(image / 400 + labels / 4)\n",
    "#plt.imshow(labels)\n",
    "plt.show()\n",
    "cl.min(), cl.max(), cl.mean(), times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(images[4])x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rr =np.array([0, 0.78, 0.78]) - r.as_euler('xyz')\n",
    "Rotation.from_euler('xyz', rr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for image, camera_pose, tool_poses, tool_meshes, times in g:\n",
    "    (x, y, z, roll, pitch, yaw) = camera_pose\n",
    "    (roll, pitch, yaw) = (radians * 180 / np.pi for radians in (roll, pitch, yaw))\n",
    "    projection = ct.RectilinearProjection(\n",
    "        focallength_px=0.5003983220157445 * 512, image=(512, 512)\n",
    "    )\n",
    "    orientation = ct.SpatialOrientation(\n",
    "        pos_x_m=x + 0.04, pos_y_m=y - 0.08, elevation_m=z + 1.0,\n",
    "        roll_deg=roll+11, tilt_deg=90-pitch, heading_deg=90-yaw,\n",
    "    )\n",
    "    camera = ct.Camera(projection, orientation)\n",
    "    print(x, y, z, roll, pitch, yaw, camera_pose[3:])\n",
    "\n",
    "    images = image_labels(camera, tool_poses, tool_meshes)\n",
    "    cl = images[0] + images[1] + images[2]\n",
    "    cl = np.broadcast_to(np.expand_dims(cl, 2), (512, 512, 3))\n",
    "    plt.imshow(image / 300 + cl)\n",
    "    plt.show()\n",
    "    cl.min(), cl.max(), cl.mean(), times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "223it [06:10,  1.66s/it]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "generator raised StopIteration",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mStopIteration\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/data/src/postprocess.py\u001b[0m in \u001b[0;36malign_generators\u001b[0;34m(key_fn, value_fn, *generators)\u001b[0m\n\u001b[1;32m    184\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_empty\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m         \u001b[0melements\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgenerators\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m         \u001b[0mkeys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0melements\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/src/postprocess.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    184\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_empty\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m         \u001b[0melements\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgenerators\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m         \u001b[0mkeys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0melements\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mStopIteration\u001b[0m: ",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-f73b84aeed39>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mevents\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mevents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1177\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1178\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1179\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m                 \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/src/postprocess.py\u001b[0m in \u001b[0;36mprocess_bag\u001b[0;34m(bag, bridge, model_topic, camera_topic, event_topic, resolution)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m     \u001b[0mi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 212\u001b[0;31m     for (tc, camera), (tp, camera_pose), (ti, rgb), (te, event) in align_generators(\n\u001b[0m\u001b[1;32m    213\u001b[0m         \u001b[0mcameras\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcamera_poses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevents\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m     ):\n",
      "\u001b[0;31mRuntimeError\u001b[0m: generator raised StopIteration"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "images = []\n",
    "events = []\n",
    "labels = []\n",
    "for image, event, label in tqdm.tqdm(g):\n",
    "    images.append(torch.tensor(image))\n",
    "    events.append(torch.tensor(event))\n",
    "    labels.append(torch.tensor(label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "223"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-7-829a83a8ca76>:1: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  torch_images = torch.stack([torch.tensor(x) for x in images])\n",
      "<ipython-input-7-829a83a8ca76>:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  torch_events = torch.stack([torch.tensor(x) for x in events])\n",
      "<ipython-input-7-829a83a8ca76>:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  torch_labels = torch.stack([torch.tensor(x) for x in labels])\n"
     ]
    }
   ],
   "source": [
    "torch_images = torch.stack([torch.tensor(x) for x in images])\n",
    "torch_events = torch.stack([torch.tensor(x) for x in events])\n",
    "torch_labels = torch.stack([torch.tensor(x) for x in labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "torchvision.io.write_video(\"h.mp4\", torch_images[4:100] / 3 + torch_labels[4:100] * 100, fps=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls ../data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls ../data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from natsort import natsorted\n",
    "\n",
    "dataset = \"../data/datasetdvs_2021-06-16-19-23-07\"\n",
    "files = !ls $dataset\n",
    "sorted_files = natsorted(files)\n",
    "len(sorted_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sorted_files' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-5a4440a8e879>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mfile_images\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mfile_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mfile\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msorted_files\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{dataset}/{file}\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mrgb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'images'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m300\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sorted_files' is not defined"
     ]
    }
   ],
   "source": [
    "import tqdm\n",
    "file_images = []\n",
    "file_labels = []\n",
    "for file in tqdm.tqdm(sorted_files):\n",
    "    with np.load(f\"{dataset}/{file}\") as data:\n",
    "        rgb = data['images'] / 300\n",
    "        label = data['labels']\n",
    "        file_images.append(rgb)\n",
    "        file_labels.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(file_labels[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(images[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels[2].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 2\n",
    "plt.imshow(labels[i].sum(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ims = torch.stack([torch.tensor(x) for x in images])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torchvision.io.write_video(\"h.mp4\", ims, fps=20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
